
> GitHub地址：https://github.com/DonFinliani/10213330403-AIfinal



---

# 当代人工智能lab5实验报告
`10213330403 新闻大数据双学位 张珈鸣`



---



# 一、作品目的
给定图片与文本数据集，设计多模态情感分析模型，完成训练与预测。

---
# 二、实验方法
## 1. 实验环境
numpy==1.25.0
opencv_python==4.9.0.80
pandas==2.0.3
Pillow==10.0.0
Pillow==10.2.0
scikit_learn==1.3.1
torch==2.1.2+cu121
torchvision==0.16.2+cu121
torchvision==0.16.2
transformers==4.36.1
~orch==2.0.1
~orch==2.1.2

---
## 2.实验文件
```c
│   dataloader.py	数据加载与预处理
│   lr_select.py	学习率搜索训练
│   main.py			multi、text、image三种模式的训练以及验证比对
│   model.py		模型构建
│   model_1.py		模型修改
│   predict.py		使用multi模型生成预测文件
│   requirements.txt	
│   train.py		具体训练方法代码
│   val.py			具体验证方法代码
│   10213330403张珈鸣lab5实验报告.md
│   README.md
│   
├───Data
│   │   predict.txt
│   │   result.txt
│   │   test_without_label.txt
│   │   train.txt
│   │   
│   ├───data     
  
├───model
│       image_7e-05_dropout0.6_batch_full.pth
│       multi_7e-05_dropout0.6_batch_full.pth
│       text_7e-05_dropout0.6_batch_full.pth
```

---
## 3.代码运行
main.py文件使用argparse包封装，参数如下
```c
parser=argparse.ArgumentParser(description="")
parser.add_argument('--mode',type=str,help='choose a mode',default='all')
parser.add_argument('--lr',type=float,help="choose learning_rate",default=7e-5)
parser.add_argument('--epochs',type=int,help="epochs num",default=5)
parser.add_argument('--dropout',type=float,help='set arg of dropout',default=0.6)
args=parser.parse_args()
```
可使用默认值运行，进行三种模式的训练与验证比对,也可输入multi、text、image进行单一模型的训练与验证。
```c
 python main.py // python --mode all --lr 7e-5 --epochs 5 --dropout 0.6
  ```
---

# 三、实验过程
## 1.模型设计
实验任务是针对一组图像和文本进行情感分类。因此可以分为两部分进行。首先对于两个不同形式的数据进行特征提取，经过组合后输入到分类器中进行三分类任务。三分类任务较为简单，因此设计一个三层的全连接层进行。对于图像和文本分别进行特征提取。
图像的特征提取采用CNN卷积神经网络，图像特征提取方面，VGGNet与ResNet都有不错的效果。采用ResNet50模型遇到cuda out of memory报错。考虑到本地GPU算力，本实验使用ResNet18进行实验。![cuda of out memory](https://img-blog.csdnimg.cn/direct/79b70948751a45428dadcea4b6a52d9e.png#pic_center)
文本的特征提取采用RNN循环神经网络，对于文本的特征提取类似于seq2seq模型中的Encoder部分，因此选择基于transformer的encoder的Bert模型进行文本特征抽取。
整体模型使用torch.nn.Module进行定义，分别定义TextEncoder、ImageEncoder、Classifier三个部分的模型，再定义CombinedModel对其进行封装。对于得到的两组不同特征，本实验采用拼贴的方式将其合并。
ImageEncoder和TextEncoder都采用了torchvision.models中的预训练模型，代码不再展示，Classifier模型结构初始建构如下
```c
        self.dropout1=nn.Dropout(self.dropout)
        self.linear1=nn.Linear(self.input_size,256)
        self.relu1=nn.ReLU()
        self.dropout2=nn.Dropout(self.dropout)
        self.linear2=nn.Linear(256,64)
        self.relu2=nn.ReLU()
        self.linear3=nn.Linear(64,3)
```
三个模型整合在CombinedModel中
```c
class CombinedModel(nn.Module):
    def __init__(self,mode,dropout=0.5):
        super(CombinedModel,self).__init__()
        self.ImageModel=ImageEncoder()
        self.TextModel=TextEncoder()
        self.ClassifyModel=Classifier(dropout,mode) 

    def forward(self,v_image,v_text,attention_mask,mode):
        if mode == 'image':
            hidden_state=self.ImageModel(v_image)
        elif mode == 'text':
            hidden_state=self.TextModel(v_text,attention_mask)
        elif mode =='multi':
            hidden_state=torch.cat((self.ImageModel(v_image),self.TextModel(v_text,attention_mask)),dim=-1)
        output=self.ClassifyModel(hidden_state)
        return output
```
## 2.数据读取与预处理
数据读取与预处理代码在dataloader.py中。
使用pandas库读取txt文件
```c
data1=pd.read_csv('Data/train.txt',index_col=0)
```
采用torch.utils.data中的父类进行数据集的构建。定义multidata子类如下
```c
class multidata(data.Dataset):
    def __init__(self,v_images,v_texts,attention_masks,labels):
        self.v_images=v_images
        self.v_texts=v_texts
        self.attention_masks=attention_masks
        self.labels=labels
    def __getitem__(self,index):
        v_image=self.v_images[index]
        v_text=self.v_texts[index]
        attention_mask=self.attention_masks[index]
        label=self.labels[index]
        return v_image,v_text,attention_mask,label
    def __len__(self):
        return len(self.labels)
```
对于图像数据，使用PIL.Image进行读取，使用torchvision.transforms进行统一的预处理。
对于文本数据，使用BertTokenizer进行词向量化。（由于文本字符较为特殊，使用UTF-8编码读取文件会报错，因此采用GB18030编码读取文件进行实验）
读取文件建立数据集之后使用torch.utils.data.DataLoader即可建立迭代器，在后续训练与验证中直接迭代使用。
```c
trainset=multidata(images_train,texts_v_train,texts_attn_train,labels_train)
valset=multidata(images_val,texts_v_val,texts_attn_val,labels_val)
trainloader=data.Dataloader(trainset,batch_size=64,shuffle=True)
valloader=data.Dataloader(valset,batch_size=64,shuffle=False)
```

## 3.训练与验证
```c
def train(model,mode,epochs=5,lr=7e-5,batch_size=64):
    start=time.perf_counter()
    criterion=nn.CrossEntropyLoss()
    model.train()
    trainloader=data.DataLoader(trainset,batch_size=batch_size,shuffle=True)
    optimizer=torch.optim.Adam(model.parameters(),lr=lr)
    losslist=[]
    acclist=[]
    for epoch in range(epochs):
        total_loss=0
        correct_num=0
        for v_images,v_texts,attention_masks,labels in trainloader:
            v_images=v_images.to(device)
            v_texts=v_texts.to(device)
            attention_masks=attention_masks.to(device)
            labels=labels.to(device)
            optimizer.zero_grad()
            outputs=model(v_images,v_texts,attention_masks,mode)
            _,preds=torch.max(outputs,dim=1)
            loss=criterion(outputs,labels)
            loss=loss.to(device)
            correct_num+=torch.sum(preds==labels)
            loss.backward()
            optimizer.step()
            total_loss+=loss
        epoch_loss=total_loss/len(trainloader)
        epoch_acc=correct_num/len(trainset)
        end_epoch=time.perf_counter()
        time_used=sec2min(end_epoch-start)
        print(f'time used:{time_used} epoch_loss:{epoch_loss:.4f} epoch_acc:{epoch_acc:.4f}')
        losslist.append(epoch_loss)
        acclist.append(epoch_acc)
    return losslist,acclist
```

```c
def val(dataloader,model,mode):
    model.eval()
    correct_num=0
    valdataloader=dataloader
    for v_images,v_texts,attention_masks,labels in valdataloader:
        v_images=v_images.to(device)
        v_texts=v_texts.to(device)
        attention_masks=attention_masks.to(device)
        labels=labels.to(device)
        with torch.no_grad():
            outputs=model(v_images,v_texts,attention_masks,mode)
            _,preds=torch.max(outputs,1)
            correct_num+=torch.sum(labels==preds)
    return correct_num.cpu().numpy() /len(valset)
```
# 四、实验结果
在train.py中编写模型训练方法train。lr_select.py中调用train函数进行学习率搜索。
首先使用小样本集进行参数调整。对于读取的dataframe格式的guid，使用.head(400)缩小数据规模,目标为在小样本集上使模型过拟合。
首先进行粗粒度搜索，设定lr_list=[1e-2,1e-3,1e-4,1e-5]
![粗粒度搜索1](https://img-blog.csdnimg.cn/direct/65755830fea2438191016fe191cd1cea.png#pic_center)
可以发现lr在[1e-3,1e-4]区间内表现较好。因此缩小搜索列表[1e-3,8e-4,6e-4,3e-4,1e-4]z再次进行训练。
![粗粒度搜索2](https://img-blog.csdnimg.cn/direct/ec59353b22164aeea34ece94faea96d3.png#pic_center)
lr在6e-4附近效果较好，最后进行一次细粒度搜索，lr_list=[6e-4,6.5e-4,7e-4],确定lr为6e-4。
![细粒度搜索](https://img-blog.csdnimg.cn/direct/39297960e0db47d2b39fc84caee0daba.png#pic_center)
使用小样本数据集进行消融实验。
![小样本消融](https://img-blog.csdnimg.cn/direct/5705b1cba5674c4a879350126d47295d.png#pic_center)
总体来看multi的效果要略优于其余两个模式，但multi的时间花费也远大于其余两个模式。

设定lr=6e-4，epochs=5，dropout=0.5，把训练数据集扩大到全集，进行消融实验。实验结果与小样本数据集不同。采用multi模式的模型预测结果反而明显低于采用单一文本或者图片数据的模型。注意到三种模式的loss都相对较高，loss下降不明显，因此猜测可能是lr设置问题。
![全集消融1](https://img-blog.csdnimg.cn/direct/a75ca8fe4c47407094f0bb2ad29679c4.png#pic_center)
对lr参数进一步补充搜索，设定lr_list=[2e-4,9e-5,7e-5],再次进行小样本训练。
![lr补](https://img-blog.csdnimg.cn/direct/1ac7f7d4c1c6499d93d7008238749da0.png#pic_center)
可以看到lr=7e-5时模型过拟合最明显，因此采用该参数再次对全集消融实验。

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/bc587cf5d7a14367a6175caf28f9390d.png#pic_center)
multi模式的预测表现有了较大提升，但对比image模式仍然不足。观察各个模式的训练与验证准确率，可以发现在全集训练上模型存在明显的过拟合。因此下一步解决模型的过拟合问题。
对于Classifier，我们加入batchnorm层。
```c
        self.dropout1=nn.Dropout(self.dropout)
        self.linear1=nn.Linear(self.input_size,256)
        self.batchnorm1=nn.BatchNorm1d(256)
        self.relu1=nn.ReLU()
        self.dropout2=nn.Dropout(self.dropout)
        self.linear2=nn.Linear(256,64)
        self.batchnorm2=nn.BatchNorm1d(64)
        self.relu2=nn.ReLU()
        self.linear3=nn.Linear(64,3)
```
同时调高dropout参数到0.6，再次对全集进行消融实验。过拟合稍有缓解，同时三个模型的验证效果都有较大提升，multi模式的表现超过了其余两种模式，因此可以判断参数以及模型调整成功。![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/ab1d53b3d7fa48d892e1066388a32f2c.png#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/419c02f6f0454828a332bd5a2475aa6b.png#pic_center)
将acc可视化，可以发现大致呈现两个acc随epoch上升的趋势。

---
## 五、模型改进

考虑到模型设计中CombinedModel的两个特征直接拼贴有些简单粗暴，因此尝试改用加权融合进行特征合并，从而优化实验。在CombinedModel中添加一个线性层以此给两个特征的结合赋予权重，相关代码文件储存在model_1.py中。
```c
        self.combine=nn.Linear(1768,1024)
		…………
  hidden_state=torch.cat((self.ImageModel(v_image),self.TextModel(v_text,attention_mask)),dim=-1)
            hidden_state=self.combine(hidden_state)
        output=self.ClassifyModel(hidden_state)
```

但最终实验结果并不理想，因此不采用。![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/b9d27b6f79254407b13decc8ff048f0f.png#pic_center)


---
